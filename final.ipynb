{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f283b87e",
   "metadata": {},
   "source": [
    "# Galaxy Classification using Supervised Learning with Deep Convolutional Neural Networks for Multi-Class Image Classification\n",
    "## Group 7 AI Class Final Projects\n",
    "Members:\n",
    "- Abi\n",
    "- Gavin\n",
    "- Rasyid\n",
    "- Hikmal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3d6ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa8bb8",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1acb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change/add any additional imports here\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Optional, forces CPU only\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7edc04",
   "metadata": {},
   "source": [
    "### Data Wrangling & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e00b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PGC_name  PGC_no     vrad  e_vrad     vopt  e_vopt        v   e_v  \\\n",
      "0  PGC0000212   212.0  11230.4     4.5  11110.0    47.0  11229.3   9.0   \n",
      "1  PGC0000218   218.0   1050.3     4.8   1027.4    25.0   1049.5   4.5   \n",
      "2  PGC0000243   243.0  -9999.0 -9999.0   8914.3    16.3   8914.3  16.3   \n",
      "3  PGC0000255   255.0    878.1     4.1  -9999.0 -9999.0    878.1   4.1   \n",
      "4  PGC0000281   281.0  -9999.0 -9999.0  11490.7    16.4  11490.7  16.4   \n",
      "\n",
      "      vvir      zvir     z_err type    objname            hl_names  \n",
      "0  11287.9  0.037650  0.000030  Sab     IC5381           PGC000212  \n",
      "1   1109.0  0.003699  0.000015  Sab    NGC7814          PGC1501809  \n",
      "2   8841.8  0.029490  0.000054   S0    NGC7808  6dFJ0003321-104441  \n",
      "3    932.9  0.003112  0.000014   Sm   UGC00017           PGC000255  \n",
      "4  11416.0  0.038080  0.000055   Sc  PGC000281       MCG-02-01-015  \n",
      "\n",
      "unique classes:\n",
      "['Sab' 'S0' 'Sm' 'Sc' 'Sa' 'Sd' 'SBa' 'IB' 'Sb' 'SABc' 'SBc' 'SBb' 'Scd'\n",
      " 'S0-a' 'I' 'SBbc' 'SBd' 'E' 'SBm' 'SABa' 'SABb' '|' 'E-SO' 'Sbc' 'SABm'\n",
      " 'SBab' 'SBcd' 'IAB' 'SABd' 'S?']\n",
      "major_class\n",
      "S        2068\n",
      "E        1097\n",
      "SB        945\n",
      "Other     348\n",
      "Name: count, dtype: int64\n",
      "      PGC_name major_class\n",
      "0   PGC0000212           S\n",
      "1   PGC0000218           S\n",
      "2   PGC0000243           E\n",
      "3   PGC0000255           S\n",
      "4   PGC0000281           S\n",
      "5   PGC0000282           S\n",
      "6   PGC0000451           S\n",
      "7   PGC0000548           S\n",
      "8   PGC0000635          SB\n",
      "9   PGC0000639           S\n",
      "10  PGC0000647       Other\n",
      "11  PGC0000670           S\n",
      "12  PGC0000825           S\n",
      "13  PGC0000963          SB\n",
      "14  PGC0001058           S\n",
      "      PGC_name major_class\n",
      "0   PGC0000212           S\n",
      "1   PGC0000218           S\n",
      "2   PGC0000243           E\n",
      "3   PGC0000255           S\n",
      "4   PGC0000281           S\n",
      "5   PGC0000282           S\n",
      "6   PGC0000451           S\n",
      "7   PGC0000548           S\n",
      "8   PGC0000635          SB\n",
      "9   PGC0000639           S\n",
      "10  PGC0000670           S\n",
      "11  PGC0000825           S\n",
      "12  PGC0000963          SB\n",
      "13  PGC0001058           S\n",
      "14  PGC0001221          SB\n"
     ]
    }
   ],
   "source": [
    "#Process the labels first\n",
    "def major_class(t):\n",
    "    t = str(t).strip()\n",
    "    if t.startswith(\"E\"):\n",
    "        return \"E\"\n",
    "    elif t.startswith(\"S0\"):\n",
    "        return \"E\" #S0 is closer to E (Lenticular)\n",
    "    elif t.startswith(\"SABa\"):\n",
    "        return \"S\" #SAB is closer to SB (Barred Spiral)\n",
    "    elif t.startswith(\"SABb\"):\n",
    "        return \"S\" #SAB is closer to SB (Barred Spiral)\n",
    "    elif t.startswith(\"SABc\"):\n",
    "        return \"SB\" #SAB is closer to SB (Barred Spiral)\n",
    "    elif t.startswith(\"SABd\"):\n",
    "        return \"SB\" #SAB is closer to SB (Barred Spiral)\n",
    "    elif t.startswith(\"SB\"):\n",
    "        return \"SB\"\n",
    "    elif t.startswith(\"S\"):\n",
    "        return \"S\"\n",
    "    else:\n",
    "        return \"Other\"  #Catchall for irregular/unknown types\n",
    "\n",
    "df = pd.read_csv('efigiuse/label.csv')\n",
    "print(df.head())\n",
    "print(\"\\nunique classes:\")\n",
    "print(df[\"type\"].unique()) #Checking the unique classes\n",
    "df[\"major_class\"] = df[\"type\"].apply(major_class)\n",
    "print(df[\"major_class\"].value_counts()) #Checking the number of major classes\n",
    "\n",
    "#Create new dataframe with only PGC_name and major_class columns\n",
    "df_major = df[[\"PGC_name\", \"major_class\"]]\n",
    "print(df_major.head(15)) #df_major at a glance\n",
    "\n",
    "#Create a filtered version of df_major (\"Other\" class removed)\n",
    "df_use = df_major[df_major[\"major_class\"] != \"Other\"].reset_index(drop=True)\n",
    "print(df_use.head(15)) #df_use at a glance\n",
    "\n",
    "#Label processing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e578b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['E' 'S' 'SB']\n"
     ]
    }
   ],
   "source": [
    "#Image processing\n",
    "image_dir = \"efigiuse/png/\"\n",
    "image_size = (255, 255)\n",
    "X, y = [], [] #Initialize empty lists for images and labels\n",
    "for idx, row in df_use.iterrows():\n",
    "    image_path = os.path.join(image_dir, f\"{row['PGC_name']}.png\")\n",
    "    if os.path.exists(image_path):\n",
    "        img = load_img(image_path, target_size=image_size)\n",
    "        img_array = img_to_array(img) / 255.0  #Normalization to [0, 1]\n",
    "        X.append(img_array) #X is for the images\n",
    "        y.append(row[\"major_class\"]) #y is for the labels\n",
    "    else:\n",
    "        print(f\"Missing image: {image_path}\")\n",
    "#No images should be missing. If any are, check efigiuse/png/ and redownload the images from: \n",
    "#https://www.astromatic.net/download/efigi/efigi_png_gri-1.6.tgz\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y) #Label encoding\n",
    "print(\"Classes: \" + str(encoder.classes_)) #Checking the classes, should be [\"E\", \"S\", \"SB\"]\n",
    "\n",
    "#Train, validation, and test split\n",
    "X = np.array(X) #Convert X & y to numpy arrays\n",
    "y_encoded = np.array(y_encoded)\n",
    "#Train (70%) - Rest (30%) split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded)\n",
    "#Validation (15%) - Test (15%) split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "#Training data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9cccf",
   "metadata": {},
   "source": [
    "### CNN Architecture Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6095247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "Epoch 1/30\n",
      "90/90 [==============================] - 120s 1s/step - loss: 1.2535 - accuracy: 0.5134 - val_loss: 15.3042 - val_accuracy: 0.2305\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 125s 1s/step - loss: 1.0956 - accuracy: 0.5255 - val_loss: 5.7019 - val_accuracy: 0.2662\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 120s 1s/step - loss: 1.0748 - accuracy: 0.5315 - val_loss: 3.5290 - val_accuracy: 0.2662\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 122s 1s/step - loss: 1.0019 - accuracy: 0.5509 - val_loss: 5.6930 - val_accuracy: 0.2662\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 118s 1s/step - loss: 0.9780 - accuracy: 0.5763 - val_loss: 1.4855 - val_accuracy: 0.3961\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 118s 1s/step - loss: 0.9491 - accuracy: 0.5728 - val_loss: 1.1240 - val_accuracy: 0.5552\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 116s 1s/step - loss: 0.9212 - accuracy: 0.5773 - val_loss: 1.0087 - val_accuracy: 0.5617\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 119s 1s/step - loss: 0.9341 - accuracy: 0.5905 - val_loss: 0.9129 - val_accuracy: 0.5844\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 120s 1s/step - loss: 0.9282 - accuracy: 0.5756 - val_loss: 0.8623 - val_accuracy: 0.6347\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 121s 1s/step - loss: 0.9506 - accuracy: 0.5742 - val_loss: 1.0457 - val_accuracy: 0.5698\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 119s 1s/step - loss: 0.9265 - accuracy: 0.5732 - val_loss: 0.8820 - val_accuracy: 0.6883\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 116s 1s/step - loss: 0.9196 - accuracy: 0.5766 - val_loss: 1.3231 - val_accuracy: 0.5682\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 114s 1s/step - loss: 0.9336 - accuracy: 0.5867 - val_loss: 0.9910 - val_accuracy: 0.6282\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 124s 1s/step - loss: 0.9120 - accuracy: 0.5937 - val_loss: 1.0622 - val_accuracy: 0.6088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_classes = len(df_use[\"major_class\"].unique()); print(\"Number of classes: \" + str(num_classes)) #Checking the number of classes\n",
    "#Current arrays are integer coded (0,1,2). Convert to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "weight_decay = 1e-4 #L2 regularization parameter\n",
    "#Class weights to handle class imbalance (especially for SB class (2))\n",
    "# class_weights = {0: 1.2486979166666667, 1: 0.7365591397849462, 2: 1.1883519206939281}\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_int),\n",
    "    y=y_train_int\n",
    ")\n",
    "# Convert to dictionary format expected by model.fit\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "#Source:\n",
    "# class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_int), y=y_train_int)\n",
    "# class_weights = dict(enumerate(class_weights_array))\n",
    "# print(\"Class weights: \", class_weights)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(255, 255, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(weight_decay), input_shape=(255, 255, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(weight_decay)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(weight_decay)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(weight_decay)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),  #Prevents overfitting\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=30,\n",
    "                    callbacks=[early_stop],\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "#WARNING: MAKE SURE TO HAVE A GENUINELY GOOD COMPUTING SETUP WITH MAXIMUM COOLING AND POWER BEFORE EXECUTING\n",
    "#DO NOT LET YOUR MACHINE GET OVERHEATED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6217ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 6s 301ms/step - loss: 0.8882 - accuracy: 0.6078\n",
      "Test Accuracy: 0.6078\n",
      "20/20 [==============================] - 6s 277ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       165\n",
      "           1       0.66      0.57      0.61       310\n",
      "           2       0.38      0.53      0.44       142\n",
      "\n",
      "    accuracy                           0.61       617\n",
      "   macro avg       0.62      0.62      0.61       617\n",
      "weighted avg       0.64      0.61      0.62       617\n",
      "\n",
      "[[124  28  13]\n",
      " [ 24 176 110]\n",
      " [  6  61  75]]\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "print(confusion_matrix(y_true, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-2.10 GPU)",
   "language": "python",
   "name": "tf-2_10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
